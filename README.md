diff --git a/README.md b/README.md
index 9d3a5f6b97a11a27a969fe918bc5d3f16379a16f..a82ac8d7c53648bab7eb74809a56344e075955e3 100644
--- a/README.md
+++ b/README.md
@@ -1,2 +1,70 @@
-# script-scrap-website
-This script was made by a developer (mansh) for scraping websites
+# Скрипт для парсинга веб-сайтов
+
+Этот проект содержит универсальный скрипт `scraper.py`, который обходит весь сайт, 
+сохраняет найденные страницы и связи между ними в базу данных SQLite, а также 
+позволяет гибко настраивать параметры сканирования.
+
+## Возможности
+
+- Ползучий обход сайта с ограничением по количеству страниц.
+- Сохранение HTML-страниц, заголовков, текста и ссылок в базу данных SQLite.
+- Поддержка задержки между запросами, таймаутов и выборочной индексации субдоменов.
+- Возможность соблюдать правила `robots.txt` и включать подробный вывод логов.
+
+## Установка зависимостей
+
+```bash
+python -m venv .venv
+source .venv/bin/activate  # в Windows: .venv\\Scripts\\activate
+pip install -r requirements.txt
+```
+
+## Использование
+
+1. Активируйте виртуальное окружение (если создавали его на шаге установки).
+2. Запустите скрипт, передав базовый URL сайта и, при необходимости, дополнительные параметры:
+
+```bash
+python scraper.py https://example.com \
+    --db-path data/example.db \
+    --max-pages 500 \
+    --delay 1.0 \
+    --timeout 15 \
+    --include-subdomains \
+    --respect-robots \
+    --verbose
+```
+
+### Основные параметры
+
+- `url` – обязательный аргумент с адресом сайта, с которого начинается обход.
+- `--db-path` – путь к файлу базы данных SQLite (по умолчанию `scraper.db`).
+- `--max-pages` – максимальное количество страниц для обработки (по умолчанию 1000).
+- `--delay` – задержка между запросами в секундах (по умолчанию 0.5).
+- `--timeout` – время ожидания ответа от сервера в секундах (по умолчанию 10).
+- `--include-subdomains` – если указать, скрипт будет переходить по ссылкам на субдомены целевого сайта.
+- `--respect-robots` – заставляет скрипт учитывать ограничения из `robots.txt` (при наличии).
+- `--verbose` – включает подробный логгинг процесса.
+
+Для просмотра всех доступных параметров можно вызвать справку:
+
+```bash
+python scraper.py --help
+```
+
+## Структура базы данных
+
+Скрипт автоматически создаёт два набора данных:
+
+- Таблица `pages` – хранит URL, статус ответа, тип контента, заголовок, текст и время сбора.
+- Таблица `links` – содержит связи между страницами (откуда и куда ведёт ссылка).
+
+## Советы по использованию
+
+- Соблюдайте правила и ограничения владельцев сайтов, которые вы анализируете.
+- При необходимости измените заголовок `User-Agent` в начале файла `scraper.py`.
+- Для крупных сайтов увеличивайте задержку (`--delay`), чтобы не перегружать сервер.
+
+## Лицензия
+
+Проект распространяется по лицензии MIT. Полный текст лицензии – в файле [LICENSE](LICENSE).
