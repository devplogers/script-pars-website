# Скрипт для парсинга веб-сайтов

Этот проект содержит универсальный скрипт `scraper.py`, который обходит весь сайт, 
сохраняет найденные страницы и связи между ними в базу данных SQLite, а также 
позволяет гибко настраивать параметры сканирования.

## Возможности

- Ползучий обход сайта с ограничением по количеству страниц.
- Сохранение HTML-страниц, заголовков, текста и ссылок в базу данных SQLite.
- Поддержка задержки между запросами, таймаутов и выборочной индексации субдоменов.
- Возможность соблюдать правила `robots.txt` и включать подробный вывод логов.

## Установка зависимостей

```bash
python -m venv .venv
source .venv/bin/activate  # в Windows: .venv\\Scripts\\activate
pip install -r requirements.txt
```

## Использование

1. Активируйте виртуальное окружение (если создавали его на шаге установки).
2. Запустите скрипт, передав базовый URL сайта и, при необходимости, дополнительные параметры:

```bash
python scraper.py https://example.com \
    --db-path data/example.db \
    --max-pages 500 \
    --delay 1.0 \
    --timeout 15 \
    --include-subdomains \
    --respect-robots \
    --verbose
```

### Основные параметры

- `url` – обязательный аргумент с адресом сайта, с которого начинается обход.
- `--db-path` – путь к файлу базы данных SQLite (по умолчанию `scraper.db`).
- `--max-pages` – максимальное количество страниц для обработки (по умолчанию 1000).
- `--delay` – задержка между запросами в секундах (по умолчанию 0.5).
- `--timeout` – время ожидания ответа от сервера в секундах (по умолчанию 10).
- `--include-subdomains` – если указать, скрипт будет переходить по ссылкам на субдомены целевого сайта.
- `--respect-robots` – заставляет скрипт учитывать ограничения из `robots.txt` (при наличии).
- `--verbose` – включает подробный логгинг процесса.

Для просмотра всех доступных параметров можно вызвать справку:

```bash
python scraper.py --help
```

## Структура базы данных

Скрипт автоматически создаёт два набора данных:

- Таблица `pages` – хранит URL, статус ответа, тип контента, заголовок, текст и время сбора.
- Таблица `links` – содержит связи между страницами (откуда и куда ведёт ссылка).

## Советы по использованию

- Соблюдайте правила и ограничения владельцев сайтов, которые вы анализируете.
- При необходимости измените заголовок `User-Agent` в начале файла `scraper.py`.
- Для крупных сайтов увеличивайте задержку (`--delay`), чтобы не перегружать сервер.

## Лицензия

Проект распространяется по лицензии MIT. Полный текст лицензии – в файле [LICENSE](LICENSE).
